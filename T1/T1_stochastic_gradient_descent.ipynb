{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from helper import *\n",
    "\n",
    "class stochastic:\n",
    "    def mse(self, x):\n",
    "        return x # TODO:\n",
    "    \n",
    "    def mse_grad(self, x):\n",
    "        return x # TODO:\n",
    "    \n",
    "    def __init__(self, X, y, method = 'mse'):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size=1\n",
    "\n",
    "        if method == 'mse':\n",
    "            self.f = self.mse \n",
    "            self.grad = self.mse_grad\n",
    "        else:\n",
    "            print('method not found')\n",
    "        \n",
    "\n",
    "    def constant_lr_scheduling(epoch, initial_lr):\n",
    "        return initial_lr\n",
    "\n",
    "    def gradient_descent(self, x0, lr_scheduling_func=constant_lr_scheduling, initial_lr=0.01, max_epochs=1000, epsilon=1e-5, minimum = 0.0, apply_min=False):\n",
    "        \"\"\"\n",
    "        Cтохастический градиентный спуск для поиска минимума функции.\n",
    "\n",
    "        Аргументы:\n",
    "            x0 (list): Начальную точка, с которой начинается поиск.\n",
    "            initial_lr (float): learning_rate - Начальная скорость обучения или шаг градиентного спуска.\n",
    "            max_epochs (int): Максимальное количество эпох или итераций для выполнения алгоритма.\n",
    "            minimum (float): Минимум функции.\n",
    "            epsilon (float): Малое число, используемое как критерий останова для алгоритма.\n",
    "        Возвращает:\n",
    "            Список всех точек, посещенных во время алгоритма.\n",
    "        \"\"\"\n",
    "        return custom_gradient_descent_with_lr_scheduling(self.f, self.grad, x0, lr_scheduling_func, initial_lr, max_epochs, epsilon, minimum, apply_min)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
