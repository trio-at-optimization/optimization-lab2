{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "Реализуйте стохастический градиентный спуск для решения линейной регрессии. Исследуйте сходимость с разным размером батча (1 - SGD, 2, $\\ldots$, $n - 1$ - Minibatch GD, $n$ - GD из предыдущей работы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..' + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from helper import *\n",
    "\n",
    "def stochastic_gradient_descent(f, initial_point, learning_rate=0.1, max_epochs=1000, minimum = 0.0, epsilon=1e-5, batch_size=1, apply_min=False):\n",
    "    \"\"\"\n",
    "    Cтохастический градиентный спуск для поиска минимума функции.\n",
    "\n",
    "    Аргументы:\n",
    "        f (function): Изначальная функция.\n",
    "        grad_fn (function): Функция, которая принимает точку и возвращает градиент в этой точке.\n",
    "        initial_point (list): Начальную точка, с которой начинается поиск.\n",
    "        learning_rate (float): Скорость обучения или шаг градиентного спуска.\n",
    "        max_epochs (int): Максимальное количество эпох или итераций для выполнения алгоритма.\n",
    "        minimum (float): Минимум функции.\n",
    "        epsilon (float): Малое число, используемое как критерий останова для алгоритма.\n",
    "        batch_size (int): кол-во координат по которым вычисляется градиент\n",
    "    Возвращает:\n",
    "        Кортеж, содержащий найденную минимальную точку, значение функции в этой точке и список всех точек, посещенных во время алгоритма.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = min(batch_size, len(initial_point))\n",
    "\n",
    "    current_point = initial_point.copy() # текущая точка, инициализируется начальной точкой\n",
    "    current_value = f(current_point) # значение функции в текущей точке\n",
    "    visited_points = [current_point.copy()] # список посещенных точек, начинается с начальной точки\n",
    "    for _ in range(max_epochs): # цикл по эпохам\n",
    "        if abs(current_value - minimum) < epsilon: # если достигнуто достаточно малое значение функции, то останавливаемся\n",
    "            break\n",
    "        prev_point = np.copy(current_point) \n",
    "        for _ in range(batch_size):\n",
    "            random_index = random.randint(0, len(current_point)-1) # выбираем случайный индекс измерения\n",
    "            gradient_random_index = fast_gradient(f, current_point, random_index) # вычисляем градиент в текущей точке в случайном индексе\n",
    "            current_point[random_index] -= learning_rate * gradient_random_index # обновляем текущую точку\n",
    "\n",
    "        new_value = f(current_point) # вычисляем значение функции в обновленной точке\n",
    "        if new_value < current_value: # если значение функции в обновленной точке меньше, чем в предыдущей, то продолжаем движение\n",
    "            current_value = new_value\n",
    "        else: # если значение функции больше или не изменилось, то возвращаемся к предыдущей точке\n",
    "            current_point = prev_point\n",
    "        visited_points.append(current_point.copy()) # добавляем текущую точку в список посещенных\n",
    "    return current_point, current_value, visited_points # возвращаем результат работы функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, epsilon=1e-5, learning_rate=0.01, max_epochs=1000, apply_min=False):\n",
    "    # X = file_info.X\n",
    "    # y = file_info.y\n",
    "    # a = file_info.real_weight\n",
    "    # b = file_info.real_bias\n",
    "\n",
    "    def mse_loss(x):\n",
    "        weights, bias = x[:-1], x[-1]\n",
    "        y_pred = np.dot(X, weights) + bias\n",
    "        mse = np.mean((y.reshape(-1, 1) - y_pred)**2)\n",
    "        return mse\n",
    "\n",
    "\n",
    "    # def mse_loss(x):\n",
    "    #     print(x)\n",
    "\n",
    "    #     weights, bias = x[:-1], x[-1]\n",
    "    #     y_pred = np.dot(X, weights) + bias\n",
    "    #     mse = np.mean((y - y_pred) ** 2)\n",
    "    #     return mse\n",
    "\n",
    "    # def mae_loss(x):\n",
    "    #     weights, bias = x[:-1], x[-1]\n",
    "    #     y_pred = np.dot(X, weights) + bias\n",
    "    #     mae = np.mean(np.abs(y - y_pred))\n",
    "    #     return mae\n",
    "\n",
    "\n",
    "    f = [mse_loss]\n",
    "    labels = ['mse_loss']\n",
    "\n",
    "    # file_info.labels_loss = labels\n",
    "\n",
    "    weight = 0\n",
    "    bias = 0\n",
    "\n",
    "    x0 = np.array([0, 0, 0, 0, 0], dtype=float)\n",
    "\n",
    "    results = []\n",
    "    count = []\n",
    "    loss_history = []\n",
    "    loss_real = []\n",
    "    \n",
    "    for i in range(len(f)):\n",
    "        point, value, result = stochastic_gradient_descent(f[i], x0, epsilon=epsilon, learning_rate=learning_rate, max_epochs=max_epochs, batch_size=2, apply_min=apply_min)\n",
    "        results.append(result)\n",
    "        count.append(len(result))\n",
    "        loss_history.append([f[i](point) for point in result])\n",
    "        # loss_real.append(f[i]([a, b]))\n",
    "    return results, count, loss_history, loss_real, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00114844 0.00118456 0.00051563 0.00089304 0.20114024]\n",
      "17.63332640173388\n",
      "1.0623710711500731\n"
     ]
    }
   ],
   "source": [
    "def random_generator(real_weights, real_bias, dots_count=100, variance=1):\n",
    "    X = np.random.rand(dots_count, len(real_weights))\n",
    "    y = real_weights * X + real_bias + (np.random.rand(dots_count, len(real_weights)) * 2 * variance - variance)\n",
    "    return X, y\n",
    "\n",
    "real_weights = [1, 2, 3, 4]\n",
    "real_bias = -1\n",
    "dots_count=50\n",
    "variance=0.25\n",
    "X, y = random_generator(real_weights, real_bias, dots_count, variance)\n",
    "\n",
    "result, count, loss_history, loss_real, labels_loss = linear_regression(X, y, epsilon=0.0085, learning_rate = 0.1, max_epochs=1000, apply_min=True) \n",
    "\n",
    "print(result[-1][-1])\n",
    "\n",
    "def mse_loss(x):\n",
    "    weights, bias = x[:-1], x[-1]\n",
    "    y_pred = np.dot(X, weights) + bias\n",
    "    mse = np.mean((y.reshape(-1, 1) - y_pred)**2)\n",
    "    return mse\n",
    "\n",
    "print(mse_loss([1, 2, 3, 4, -1]))\n",
    "print(mse_loss(result[-1][-1]))\n",
    "\n",
    "# print_2d_data(f_info, [[[[2, -1]]]], ['Mini-batch'], show_print=True)\n",
    "# print_loss(loss_history, loss_real, labels_loss)\n",
    "# print_2d_function(loss_history, labels_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
