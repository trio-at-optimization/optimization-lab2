{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "Реализуйте стохастический градиентный спуск для решения линейной регрессии. Исследуйте сходимость с разным размером батча (1 - SGD, 2, $\\ldots$, $n - 1$ - Minibatch GD, $n$ - GD из предыдущей работы)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "CNST_DPI_HQ = 1024\n",
    "\n",
    "def exp_decay(epoch, k, initial_lrate=0.3):\n",
    "   return initial_lrate * math.exp(-0.05 * epoch)\n",
    "\n",
    "def const_decay(_, __, initial_lrate=0.3):\n",
    "   return initial_lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from helper import *\n",
    "\n",
    "def stochastic_gradient_descent(f, initial_point, learning_rate_func, learning_rate=0.1, max_epochs=1000, minimum = 0.0, epsilon=0.1, batch_size=1):\n",
    "    \"\"\"\n",
    "    Cтохастический градиентный спуск для поиска минимума функции c learning rate scheduling.\n",
    "\n",
    "    Аргументы:\n",
    "        f (function): Изначальная функция.\n",
    "        grad_fn (function): Функция, которая принимает точку и возвращает градиент в этой точке.\n",
    "        initial_point (list): Начальную точка, с которой начинается поиск.\n",
    "        learning_rate_func (function): learning rate scheduling function.\n",
    "        learning_rate (float): Скорость обучения или шаг градиентного спуска.\n",
    "        max_epochs (int): Максимальное количество эпох или итераций для выполнения алгоритма.\n",
    "        minimum (float): Минимум функции.\n",
    "        epsilon (float): Малое число, используемое как критерий останова для алгоритма.\n",
    "        batch_size (int): кол-во координат по которым вычисляется градиент\n",
    "    Возвращает:\n",
    "        Число пройденных шагов.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = min(batch_size, len(initial_point))\n",
    "\n",
    "    current_point = initial_point.copy()\n",
    "    current_value = f(current_point)\n",
    "    steps_count = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        if abs(current_value - minimum) < epsilon:\n",
    "            break\n",
    "\n",
    "        prev_point = np.copy(current_point)\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            random_index = random.randint(0, len(current_point)-1) \n",
    "            gradient_random_index = fast_gradient(f, current_point, random_index) \n",
    "            current_point[random_index] -= learning_rate_func(epoch, batch_size, learning_rate) * gradient_random_index\n",
    "\n",
    "        new_value = f(current_point)\n",
    "\n",
    "        if new_value < current_value:\n",
    "            current_value = new_value\n",
    "        else:\n",
    "            current_point = prev_point\n",
    "\n",
    "        steps_count += 1\n",
    "        \n",
    "    return steps_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_const_decay_with_exp_decay(f, lr_step_size=1e-1, max_epochs=10000, filename='', filename_extension='.png', dpi=CNST_DPI_HQ):\n",
    "    lr_values = np.arange(0, 100, lr_step_size)\n",
    "    x0 = np.array([0, 0], dtype=float)\n",
    "    batch_size = 2\n",
    "    learning_rate_functions = [const_decay, exp_decay]\n",
    "    labels = ['Constant',\n",
    "              'Exponential']\n",
    "    \n",
    "\n",
    "    for i in range(2):\n",
    "        learning_rate_function = learning_rate_functions[i]\n",
    "        result = []\n",
    "    \n",
    "        for lr_iter in tqdm(lr_values):\n",
    "            steps = stochastic_gradient_descent(f, x0, learning_rate_function, learning_rate=lr_iter, max_epochs=max_epochs, batch_size=batch_size)\n",
    "            result.append(steps)\n",
    "\n",
    "        plt.xlabel('Learning rate', fontsize=14)\n",
    "        plt.ylabel('Steps', fontsize=14)\n",
    "        plt.plot(lr_values, result, label=labels[i])\n",
    "\n",
    "    # min_lr_index = np.argmin(result)\n",
    "    # min_steps = result[min_lr_index]\n",
    "    # min_lr = lr_values[min_lr_index]\n",
    "\n",
    "    # plt.annotate(f'[{min_lr}, {min_steps}]', \n",
    "    #             xy=(min_lr, min_steps),\n",
    "    #             xytext=(min_lr-0.1, min_steps+max_iter/10),\n",
    "    #             bbox=dict(boxstyle=\"round\", fc=\"0.8\"),\n",
    "    #             arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "    \n",
    "    # plt.xlabel('Learning rate', fontsize=14)\n",
    "    # plt.ylabel('Steps', fontsize=14)\n",
    "    # plt.plot(lr_values, result)\n",
    "    \n",
    "    if(filename != ''):\n",
    "        plt.savefig(filename + filename_extension, dpi=dpi, bbox_inches=0, transparent=True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование и отладка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(x):\n",
    "        weights, bias = x[0], x[1]\n",
    "        y_pred = np.dot(X, weights) + bias\n",
    "        mse = np.mean((y - y_pred) ** 2)\n",
    "        return mse\n",
    "\n",
    "\n",
    "# ======== style-parameters\n",
    "plt.style.use('default')\n",
    "_ = plt.figure(figsize=(8, 8))\n",
    "# =========================\n",
    "\n",
    "# Генерируем случайные точки\n",
    "real_weight, real_bias = 2, 0\n",
    "\n",
    "dots_count = 500\n",
    "variance = 3\n",
    "X = np.random.rand(dots_count, 1)\n",
    "y = real_weight * X + real_bias + (np.random.rand(dots_count, 1) * variance - variance / 2)\n",
    "    \n",
    "compare_const_decay_with_exp_decay(mse_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant vs Exponential\n",
      "LR: 45\n",
      "Const: 10000\n",
      "Exponential: 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mse_loss(x):\n",
    "        weights, bias = x[0], x[1]\n",
    "        y_pred = np.dot(X, weights) + bias\n",
    "        mse = np.mean((y - y_pred) ** 2)\n",
    "        return mse\n",
    "\n",
    "\n",
    "# ======== style-parameters\n",
    "plt.style.use('default')\n",
    "_ = plt.figure(figsize=(8, 8))\n",
    "# =========================\n",
    "\n",
    "# Генерируем случайные точки\n",
    "real_weight, real_bias = 2, 0\n",
    "\n",
    "dots_count = 500\n",
    "variance = 3\n",
    "X = np.random.rand(dots_count, 1)\n",
    "y = real_weight * X + real_bias + (np.random.rand(dots_count, 1) * variance - variance / 2)\n",
    "\n",
    "lr = 45\n",
    "batch_size = 2\n",
    "max_epochs = 10_000\n",
    "x0 = np.array([0, 0], dtype=float)\n",
    "epsilon = 1e-1\n",
    "    \n",
    "const = stochastic_gradient_descent(mse_loss, x0, const_decay, learning_rate=lr, max_epochs=max_epochs, epsilon=epsilon, batch_size=batch_size)\n",
    "exp = stochastic_gradient_descent(mse_loss, x0, exp_decay, learning_rate=lr, max_epochs=max_epochs, batch_size=batch_size)\n",
    "\n",
    "print(f'Constant vs Exponential\\nLR: {lr}\\nConst: {const}\\nExponential: {exp}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
