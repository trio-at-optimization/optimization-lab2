{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class minibatch:\n",
    "    def mse_loss(self, x):\n",
    "        y_pred = np.dot(self.X, x)\n",
    "        mse = np.mean((self.y - y_pred) ** 2)\n",
    "        return mse\n",
    "\n",
    "    def mse_loss_grad(self, x):\n",
    "        # print(self.X.shape, self.y.shape)\n",
    "\n",
    "        # Choose n random data points from the training set without replacement\n",
    "        indices = np.random.choice(self.X.shape[0], self.batch_size, replace=False)\n",
    "        X_batch = self.X[indices, :]\n",
    "        y_batch = self.y[indices]\n",
    "\n",
    "        # print(X_batch.shape, y_batch.shape)\n",
    "\n",
    "        # Compute the gradient of the MSE loss with respect to x for the chosen data points\n",
    "        y_pred = np.dot(X_batch, x)\n",
    "        grad = 2 * np.dot(X_batch.T, (y_pred - y_batch))\n",
    "\n",
    "        # print(grad)\n",
    "\n",
    "        # Sum values in rows of grad and divide by n\n",
    "        # grad_mean = np.sum(grad, axis=1) / self.batch_size\n",
    "\n",
    "        # print(grad_mean)\n",
    "        return grad\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=2, method='mse'):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if self.batch_size > X.shape[0]:\n",
    "            self.batch_size = X.shape[0]\n",
    "\n",
    "        if method == 'mse':\n",
    "            self.f = self.mse_loss \n",
    "            self.grad = self.mse_loss_grad\n",
    "        else:\n",
    "            print('method not found')\n",
    "        \n",
    "\n",
    "    def constant_lr_scheduling(epoch, initial_lr):\n",
    "        return initial_lr\n",
    "\n",
    "    def gradient_descent(self, x0, lr_scheduling_func=constant_lr_scheduling, initial_lr=0.001, max_epochs=100, eps=1e-5, minimum = 0.0, apply_min=False, apply_value=True):\n",
    "        \"\"\"\n",
    "        Cтохастический градиентный спуск для поиска минимума функции.\n",
    "\n",
    "        Аргументы:\n",
    "            x0 (list): Начальную точка, с которой начинается поиск.\n",
    "            initial_lr (float): learning_rate - Начальная скорость обучения или шаг градиентного спуска.\n",
    "            max_epochs (int): Максимальное количество эпох или итераций для выполнения алгоритма.\n",
    "            minimum (float): Минимум функции.\n",
    "            epsilon (float): Малое число, используемое как критерий останова для алгоритма.\n",
    "        Возвращает:\n",
    "            Список всех точек, посещенных во время алгоритма.\n",
    "        \"\"\"\n",
    "        return custom_gradient_descent_with_lr_scheduling(self.f, self.grad, x0, lr_scheduling_func, initial_lr, max_epochs, eps, minimum, apply_min, apply_value)\n",
    "    \n",
    "    def get_loss_history(self, results):\n",
    "        loss_history = []\n",
    "\n",
    "        for i in range(len(results)):\n",
    "            loss_history.append(self.f(results[i]))\n",
    "\n",
    "        return loss_history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_model_LinearRegression:\n",
    "    def __init__(self, batch_size=None):\n",
    "        self.coef_ = []\n",
    "        self.intercept_ = None\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X_train, y_train, epsilon=24, learning_rate=0.001, max_epochs=100, batch_size=1, apply_min=True):\n",
    "        X = X_train \n",
    "        y = y_train\n",
    "\n",
    "        if self.batch_size is not None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        mb_gd = minibatch(X, y, batch_size=batch_size)\n",
    "        x0 = np.zeros(X.shape[1], dtype=float)\n",
    "\n",
    "        results = mb_gd.gradient_descent(x0, max_epochs=1000, initial_lr=1e-8, eps=70, apply_min=True, apply_value=True)\n",
    "\n",
    "        self.coef_ = results[-1][:-1]\n",
    "        self.intercept_ = results[-1][-1] \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "\n",
    "        for i in range(len(X_test)):\n",
    "            y = self.intercept_ + sum([self.coef_[j] * float(X_test[i][j]) for j in range(len(self.coef_))])\n",
    "            y_pred.append(y)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_openml\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_reference_result(dataset):\n",
    "    model = LinearRegression()\n",
    "    return train_test_print_model(model, dataset, print_result=False, print_info=False, view_graphics=False)\n",
    "\n",
    "def get_idea_result():\n",
    "    return 0.0, 1.0\n",
    "\n",
    "def research_LinearRegression_calculation(model_linear_regression):\n",
    "    dataset = fetch_openml(name='boston', version=1, as_frame=True , parser='liac-arff')\n",
    "    mse_reference, r2_reference = get_reference_result(dataset)\n",
    "    mse_ideal, r2_ideal = get_idea_result()\n",
    "\n",
    "    print(f\"{'mse_reference:':<15}{mse_reference:<14.6g} {' r2_reference:':<15}{r2_reference:<14.6g}\")\n",
    "    print(f\"{'mse_ideal:':<15}{mse_ideal:<14.6g} {' r2_ideal:':<15}{r2_ideal:<14.6g}\")\n",
    "\n",
    "    mse_results = []\n",
    "    r2_results = []\n",
    "    k = 10\n",
    "\n",
    "    for i in range(1, dataset.data.values.shape[1] + 1):\n",
    "        model = model_linear_regression(batch_size=i)\n",
    "        mse = mse_reference/k\n",
    "        r2 = r2_reference/k\n",
    "        tries = 0\n",
    "        # pbar = tqdm(desc=f\"Batch size: {i}\", unit=\" test\")\n",
    "        while mse <= mse_reference/k or r2 <= r2_reference/k:    \n",
    "            mse, r2 = train_test_print_model(model, dataset, print_result=False, view_graphics=False)\n",
    "            tries += 1\n",
    "            print(f\"Batch size: {i}, Try: {tries}, MSE: {mse:.2f}, R^2: {r2:.2f}\")\n",
    "            # pbar.update()\n",
    "        # pbar.close()\n",
    "\n",
    "        mse_results.append(mse)\n",
    "        r2_results.append(r2)\n",
    "    \n",
    "    return mse_results, r2_results, mse_reference, r2_reference, mse_ideal, r2_ideal\n",
    "\n",
    "def research_LinearRegression_view(mse_results, r2_results, mse_reference, r2_reference, mse_ideal, r2_ideal):\n",
    "    # Строим графики\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # График MSE\n",
    "    axs[0].plot(range(1, len(mse_results)+1), mse_results, marker='o')\n",
    "    axs[0].set_xlabel('Batch size')\n",
    "    axs[0].set_ylabel('MSE')\n",
    "    axs[0].axhline(y=mse_reference, color='y', linestyle='--', label='MSE reference')\n",
    "    axs[0].axhline(y=mse_ideal, color='g', linestyle='--', label='MSE ideal')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # График R^2\n",
    "    axs[1].plot(range(1, len(r2_results)+1), r2_results, marker='o')\n",
    "    axs[1].set_xlabel('Batch size')\n",
    "    axs[1].set_ylabel('R^2')\n",
    "    axs[1].axhline(y=r2_reference, color='y', linestyle='--', label='R^2 reference')\n",
    "    axs[1].axhline(y=r2_ideal, color='g', linestyle='--', label='R^2 ideal')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def research_LinearRegression(model_linear_regression):\n",
    "    mse_results, r2_results, mse_reference, r2_reference, mse_ideal, r2_ideal = research_LinearRegression_calculation(model_linear_regression)\n",
    "    research_LinearRegression_view(mse_results, r2_results, mse_reference, r2_reference, mse_ideal, r2_ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_reference: 24.2911         r2_reference: 0.668759      \n",
      "mse_ideal:     0               r2_ideal:     1             \n",
      "Batch size: 1, Try: 1, MSE: 97.94, R^2: -0.34\n",
      "Batch size: 1, Try: 2, MSE: 95.42, R^2: -0.30\n",
      "Batch size: 1, Try: 3, MSE: 94.87, R^2: -0.29\n",
      "Batch size: 1, Try: 4, MSE: 98.82, R^2: -0.35\n",
      "Batch size: 1, Try: 5, MSE: 95.24, R^2: -0.30\n",
      "Batch size: 1, Try: 6, MSE: 99.12, R^2: -0.35\n",
      "Batch size: 1, Try: 7, MSE: 95.92, R^2: -0.31\n",
      "Batch size: 1, Try: 8, MSE: 96.38, R^2: -0.31\n",
      "Batch size: 1, Try: 9, MSE: 96.55, R^2: -0.32\n",
      "Batch size: 1, Try: 10, MSE: 95.56, R^2: -0.30\n",
      "Batch size: 1, Try: 11, MSE: 97.43, R^2: -0.33\n",
      "Batch size: 1, Try: 12, MSE: 98.24, R^2: -0.34\n",
      "Batch size: 1, Try: 13, MSE: 97.40, R^2: -0.33\n",
      "Batch size: 1, Try: 14, MSE: 94.99, R^2: -0.30\n",
      "Batch size: 1, Try: 15, MSE: 96.78, R^2: -0.32\n",
      "Batch size: 1, Try: 16, MSE: 97.91, R^2: -0.34\n",
      "Batch size: 1, Try: 17, MSE: 98.04, R^2: -0.34\n",
      "Batch size: 1, Try: 18, MSE: 95.83, R^2: -0.31\n",
      "Batch size: 1, Try: 19, MSE: 97.47, R^2: -0.33\n",
      "Batch size: 1, Try: 20, MSE: 98.58, R^2: -0.34\n",
      "Batch size: 1, Try: 21, MSE: 97.31, R^2: -0.33\n",
      "Batch size: 1, Try: 22, MSE: 98.71, R^2: -0.35\n",
      "Batch size: 1, Try: 23, MSE: 94.12, R^2: -0.28\n",
      "Batch size: 1, Try: 24, MSE: 99.36, R^2: -0.35\n",
      "Batch size: 1, Try: 25, MSE: 97.62, R^2: -0.33\n",
      "Batch size: 1, Try: 26, MSE: 97.26, R^2: -0.33\n",
      "Batch size: 1, Try: 27, MSE: 98.51, R^2: -0.34\n",
      "Batch size: 1, Try: 28, MSE: 96.76, R^2: -0.32\n",
      "Batch size: 1, Try: 29, MSE: 96.09, R^2: -0.31\n",
      "Batch size: 1, Try: 30, MSE: 97.72, R^2: -0.33\n",
      "Batch size: 1, Try: 31, MSE: 98.36, R^2: -0.34\n",
      "Batch size: 1, Try: 32, MSE: 97.53, R^2: -0.33\n",
      "Batch size: 1, Try: 33, MSE: 96.94, R^2: -0.32\n",
      "Batch size: 1, Try: 34, MSE: 98.95, R^2: -0.35\n",
      "Batch size: 1, Try: 35, MSE: 96.50, R^2: -0.32\n",
      "Batch size: 1, Try: 36, MSE: 95.86, R^2: -0.31\n",
      "Batch size: 1, Try: 37, MSE: 96.86, R^2: -0.32\n",
      "Batch size: 1, Try: 38, MSE: 95.91, R^2: -0.31\n",
      "Batch size: 1, Try: 39, MSE: 96.99, R^2: -0.32\n",
      "Batch size: 1, Try: 40, MSE: 96.30, R^2: -0.31\n",
      "Batch size: 1, Try: 41, MSE: 97.06, R^2: -0.32\n",
      "Batch size: 1, Try: 42, MSE: 99.22, R^2: -0.35\n",
      "Batch size: 1, Try: 43, MSE: 97.48, R^2: -0.33\n",
      "Batch size: 1, Try: 44, MSE: 97.32, R^2: -0.33\n",
      "Batch size: 1, Try: 45, MSE: 96.13, R^2: -0.31\n",
      "Batch size: 1, Try: 46, MSE: 98.75, R^2: -0.35\n",
      "Batch size: 1, Try: 47, MSE: 94.68, R^2: -0.29\n",
      "Batch size: 1, Try: 48, MSE: 96.90, R^2: -0.32\n",
      "Batch size: 1, Try: 49, MSE: 99.00, R^2: -0.35\n",
      "Batch size: 1, Try: 50, MSE: 96.39, R^2: -0.31\n",
      "Batch size: 1, Try: 51, MSE: 97.49, R^2: -0.33\n",
      "Batch size: 1, Try: 52, MSE: 96.24, R^2: -0.31\n",
      "Batch size: 1, Try: 53, MSE: 95.93, R^2: -0.31\n",
      "Batch size: 1, Try: 54, MSE: 97.82, R^2: -0.33\n",
      "Batch size: 1, Try: 55, MSE: 99.35, R^2: -0.35\n",
      "Batch size: 1, Try: 56, MSE: 96.69, R^2: -0.32\n",
      "Batch size: 1, Try: 57, MSE: 96.90, R^2: -0.32\n",
      "Batch size: 1, Try: 58, MSE: 97.53, R^2: -0.33\n",
      "Batch size: 1, Try: 59, MSE: 96.85, R^2: -0.32\n",
      "Batch size: 1, Try: 60, MSE: 97.87, R^2: -0.33\n",
      "Batch size: 1, Try: 61, MSE: 99.06, R^2: -0.35\n",
      "Batch size: 1, Try: 62, MSE: 96.55, R^2: -0.32\n",
      "Batch size: 1, Try: 63, MSE: 98.57, R^2: -0.34\n",
      "Batch size: 1, Try: 64, MSE: 96.35, R^2: -0.31\n",
      "Batch size: 1, Try: 65, MSE: 99.52, R^2: -0.36\n",
      "Batch size: 1, Try: 66, MSE: 96.63, R^2: -0.32\n",
      "Batch size: 1, Try: 67, MSE: 97.43, R^2: -0.33\n",
      "Batch size: 1, Try: 68, MSE: 97.59, R^2: -0.33\n",
      "Batch size: 1, Try: 69, MSE: 98.31, R^2: -0.34\n",
      "Batch size: 1, Try: 70, MSE: 98.26, R^2: -0.34\n",
      "Batch size: 1, Try: 71, MSE: 97.72, R^2: -0.33\n",
      "Batch size: 1, Try: 72, MSE: 98.19, R^2: -0.34\n",
      "Batch size: 1, Try: 73, MSE: 99.50, R^2: -0.36\n",
      "Batch size: 1, Try: 74, MSE: 97.45, R^2: -0.33\n",
      "Batch size: 1, Try: 75, MSE: 99.04, R^2: -0.35\n",
      "Batch size: 1, Try: 76, MSE: 94.63, R^2: -0.29\n",
      "Batch size: 1, Try: 77, MSE: 98.60, R^2: -0.34\n",
      "Batch size: 1, Try: 78, MSE: 96.18, R^2: -0.31\n",
      "Batch size: 1, Try: 79, MSE: 99.22, R^2: -0.35\n",
      "Batch size: 1, Try: 80, MSE: 99.48, R^2: -0.36\n",
      "Batch size: 1, Try: 81, MSE: 95.96, R^2: -0.31\n",
      "Batch size: 1, Try: 82, MSE: 98.36, R^2: -0.34\n",
      "Batch size: 1, Try: 83, MSE: 94.68, R^2: -0.29\n",
      "Batch size: 1, Try: 84, MSE: 97.58, R^2: -0.33\n",
      "Batch size: 1, Try: 85, MSE: 99.00, R^2: -0.35\n",
      "Batch size: 1, Try: 86, MSE: 96.84, R^2: -0.32\n",
      "Batch size: 1, Try: 87, MSE: 98.91, R^2: -0.35\n",
      "Batch size: 1, Try: 88, MSE: 96.75, R^2: -0.32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m research_LinearRegression(get_model_LinearRegression)\n",
      "Cell \u001b[1;32mIn[176], line 65\u001b[0m, in \u001b[0;36mresearch_LinearRegression\u001b[1;34m(model_linear_regression)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresearch_LinearRegression\u001b[39m(model_linear_regression):\n\u001b[1;32m---> 65\u001b[0m     mse_results, r2_results, mse_reference, r2_reference, mse_ideal, r2_ideal \u001b[39m=\u001b[39m research_LinearRegression_calculation(model_linear_regression)\n\u001b[0;32m     66\u001b[0m     research_LinearRegression_view(mse_results, r2_results, mse_reference, r2_reference, mse_ideal, r2_ideal)\n",
      "Cell \u001b[1;32mIn[176], line 31\u001b[0m, in \u001b[0;36mresearch_LinearRegression_calculation\u001b[1;34m(model_linear_regression)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m# pbar = tqdm(desc=f\"Batch size: {i}\", unit=\" test\")\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mwhile\u001b[39;00m mse \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m mse_reference\u001b[39m/\u001b[39mk \u001b[39mor\u001b[39;00m r2 \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m r2_reference\u001b[39m/\u001b[39mk:    \n\u001b[1;32m---> 31\u001b[0m     mse, r2 \u001b[39m=\u001b[39m train_test_print_model(model, dataset, print_result\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, view_graphics\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     32\u001b[0m     tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     33\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch size: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m, Try: \u001b[39m\u001b[39m{\u001b[39;00mtries\u001b[39m}\u001b[39;00m\u001b[39m, MSE: \u001b[39m\u001b[39m{\u001b[39;00mmse\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, R^2: \u001b[39m\u001b[39m{\u001b[39;00mr2\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\My\\Desktop\\GitRepo\\metopt-lab2\\helper\\package_T1\\module_dataset.py:84\u001b[0m, in \u001b[0;36mtrain_test_print_model\u001b[1;34m(model, dataset, X, y, print_info, print_result, view_graphics, title)\u001b[0m\n\u001b[0;32m     80\u001b[0m     y \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mtarget\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m     82\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     86\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     88\u001b[0m mse \u001b[39m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "Cell \u001b[1;32mIn[175], line 17\u001b[0m, in \u001b[0;36mget_model_LinearRegression.fit\u001b[1;34m(self, X_train, y_train, epsilon, learning_rate, max_epochs, batch_size, apply_min)\u001b[0m\n\u001b[0;32m     14\u001b[0m mb_gd \u001b[39m=\u001b[39m minibatch(X, y, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[0;32m     15\u001b[0m x0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m results \u001b[39m=\u001b[39m mb_gd\u001b[39m.\u001b[39;49mgradient_descent(x0, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, initial_lr\u001b[39m=\u001b[39;49m\u001b[39m1e-8\u001b[39;49m, eps\u001b[39m=\u001b[39;49m\u001b[39m70\u001b[39;49m, apply_min\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, apply_value\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_ \u001b[39m=\u001b[39m results[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_ \u001b[39m=\u001b[39m results[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[174], line 60\u001b[0m, in \u001b[0;36mminibatch.gradient_descent\u001b[1;34m(self, x0, lr_scheduling_func, initial_lr, max_epochs, eps, minimum, apply_min, apply_value)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient_descent\u001b[39m(\u001b[39mself\u001b[39m, x0, lr_scheduling_func\u001b[39m=\u001b[39mconstant_lr_scheduling, initial_lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, max_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, eps\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m, minimum \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, apply_min\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, apply_value\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     48\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m    Cтохастический градиентный спуск для поиска минимума функции.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m        Список всех точек, посещенных во время алгоритма.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[39mreturn\u001b[39;00m custom_gradient_descent_with_lr_scheduling(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad, x0, lr_scheduling_func, initial_lr, max_epochs, eps, minimum, apply_min, apply_value)\n",
      "File \u001b[1;32mc:\\Users\\My\\Desktop\\GitRepo\\metopt-lab2\\helper\\package_T1\\module_minibatch_gradient_descent.py:58\u001b[0m, in \u001b[0;36mcustom_gradient_descent_with_lr_scheduling\u001b[1;34m(f, gradient, x0, lr_scheduling_func, initial_lr, num_iterations, eps, minimum, apply_min, apply_value)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mif\u001b[39;00m apply_min \u001b[39mand\u001b[39;00m \u001b[39mabs\u001b[39m(f(x) \u001b[39m-\u001b[39m minimum) \u001b[39m<\u001b[39m eps:\n\u001b[0;32m     56\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m grad_x \u001b[39m=\u001b[39m gradient(x)\n\u001b[0;32m     59\u001b[0m new_x \u001b[39m=\u001b[39m x \u001b[39m-\u001b[39m grad_x \u001b[39m*\u001b[39m lr_scheduling_func(i, initial_lr)\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m apply_value:\n",
      "Cell \u001b[1;32mIn[174], line 18\u001b[0m, in \u001b[0;36mminibatch.mse_loss_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m y_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my[indices]\n\u001b[0;32m     15\u001b[0m \u001b[39m# print(X_batch.shape, y_batch.shape)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39m# Compute the gradient of the MSE loss with respect to x for the chosen data points\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(X_batch, x)\n\u001b[0;32m     19\u001b[0m grad \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mdot(X_batch\u001b[39m.\u001b[39mT, (y_pred \u001b[39m-\u001b[39m y_batch))\n\u001b[0;32m     21\u001b[0m \u001b[39m# print(grad)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[39m# Sum values in rows of grad and divide by n\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# grad_mean = np.sum(grad, axis=1) / self.batch_size\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[39m# print(grad_mean)\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "research_LinearRegression(get_model_LinearRegression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
