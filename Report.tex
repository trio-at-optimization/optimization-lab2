\documentclass[12pt, a4paper, oneside, final]{article}
\usepackage[margin = 1in, bottom = 1in]{geometry}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{xcolor, ulem, soulutf8, soul, fancyhdr, amsmath, amssymb, amsthm}
\usepackage[shortlabels]{enumitem}
\usepackage{titlesec, hyperref, multicol, listings}
\usepackage[most]{tcolorbox}
\usepackage[makeroom]{cancel}
\usepackage[most]{tcolorbox}
\usepackage{tocloft, longtable, skak, stmaryrd, color}
\usepackage[framemethod = tikz]{mdframed}
\usepackage{svg, wrapfig, csvsimple, float, caption, subcaption}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{blueish}{rgb}{0.96,0.96,1.0}
\definecolor{grayblueish}{rgb}{0.97,0.97,0.98}
\definecolor{transblue}{rgb}{0.9,0.9,0.97}
\definecolor{transred}{rgb}{0.97,0.9,0.9}
\definecolor{light-gray}{gray}{0.95}

\renewcommand{\figurename}{}
\addto\captionsrussian{\renewcommand{\figurename}{}}
\captionsetup[table]{labelformat = empty}

\hypersetup{
	colorlinks,
	citecolor = pink,
	filecolor = pink,
	linkcolor = pink,
	urlcolor = pink
}

\newtcblisting{mylisting}{
	listing only,
	breakable,
	colback=backcolour,
	enhanced jigsaw,
	sharp corners,
	boxrule=0pt,
	frame hidden,
	listing options={
		mathescape,
		commentstyle = \color{codegreen},
		keywordstyle = \color{magenta},
		numberstyle = \tiny\color{codegray},
		stringstyle = \color{codepurple},
		basicstyle = \ttfamily\footnotesize,
		breakatwhitespace = false,
		breaklines = true,
		captionpos = b,
		keepspaces = true,
		numbers = left,
		numbersep = 5pt,
		showspaces = false,
		showstringspaces = false,
		showtabs = false,
		tabsize = 4,
		inputencoding=utf8,
		language = python
	}
}

\binoppenalty = 10000
\relpenalty = 10000
\sloppy

\renewcommand*{\theenumi}{\thesection.\arabic{enumi}}
\renewcommand*{\theenumii}{\alph{enumii}}
\renewcommand*{\labelitemi}{\ensuremath{\triangleright}}

\everymath{\displaystyle}

\begin{document}
	\thispagestyle{empty}
	\vspace*{0.5em}
	\begin{center}
		{Национальный исследовательский университет ИТМО\\Факультет информационных технологий и программирования\\Прикладная математика и информатика}\\[5.0em]
		{\Huge \bfseries Методы оптимизации}\\[0.5em]
		{\large Отчет по лабораторной работе №2}\\[0.5em]
		\textcolor{gray}{\textlangle Собрано \today\textrangle}
	\end{center}
	\begingroup
	\def\hd{\begin{tabular}{ll}
			\textbf{Работу выполнили:} \\ Бактурин Савелий Филиппович M32331 \\ Вереня Андрей Тарасович M32331 \\ Сотников Максим Владимирович M32331 \vspace*{1em} \\
			\textbf{Преподаватель:} \\ TBA
		\end{tabular}
	}
	\vspace*{30em}
	\newlength{\hdwidth}
	\settowidth{\hdwidth}{\hd}
	\hfill\begin{minipage}{\hdwidth}\hd\end{minipage}
	\endgroup
	\newpage
	\section*{Стохастический градиентный спуск}
	\subsection*{Исследование с разными размерами батча}
	Реализуйте стохастический градиентный спуск для решения линейной регрессии.
	Исследуйте сходимость с разным размером батча ($1$~-- SGD, $2$, $\dots$, $n - 1$~-- Minibatch GD, $n$~-- GD из предыдущей работы).
	\subsubsection*{Стохастический градиентный спуск}
	\textit{Стохастический градиентный спуск}~--- модификация к основному методу итерационного поиска минимума через антиградиент дифференцируемой функции в рассматриваемой плоскости $\mathbb{R}^{n}$.
	Идея: пусть есть множество $M$~-- какой-то полный набор данных вычисленных оценок при спуске к минимуму, тогда в рассматриваемой версии мы будем брать случайное значение из выбранного $M' \subset M$.
	Как правило, такой подход преуменьшает вычислительные ресурсы, в особенности, следует помнить, что $\mathtt{float}$ считается крайне медленно, и ускоряет итерацию по количествам эпохам, но при этом мы теряем точность сходимости.

	Пусть $x_{i} = \{x_i^0, x_i^1, ~ \ldots, x_i^{n - 1}\}$~-- координата в $\mathbb{R}^{n}$ и задана функция $f(x_i) : \mathbb{R}^{n} \to \mathbb{R}$.
	Мы хотим нашу задачу свести к исследованию на некоторых специальных образцах заданной функции, для каждой точки из которых мы будем минимизировать ошибку для дальнейшего нахождения приближенного минимума рассматриваемой функции $f(x_i)$.
	Мы хотим найти линейную регрессию, представляющая из себя полином $1$-ой степени от $n$ переменных.
	Для начала мы найдем функцию ошибки $S$ по следующей формуле:
	\[
		S(f) = (X'^{\mathrm{T}} \times X')^{-1} \times X'^{\mathrm{T}} \times Y \times \vec{x},
	\] где $Y$~-- матрица значений при множестве $X$ (определение), $X'$~-- это матрица $X$, но в $1$-ой колонке забитый единицами.
	По другому мы можем записать данную формулу следующим образом:
	\[
		S(f) = \sum\limits_{i = 1}^{N}{\left(y_{i} - x_{i} \cdot w_{i}\right)^{2}},
	\] где $y_{i} \in D(f(x_{i}))$ (образ функции), $w_{i} \in W$~-- сгенерированные веса, коэффициенты при линейной функции.

	Рассмотрим идею алгоритма.
	При итерации, пока мы не превысили максимальное количество шагов или не сведем нашу функцию потери до некоторого $\varepsilon$, мы будем обобщать экспериментальные данные в виде случайных точек в некоторую многомерную линию.
	На каждом шаге мы будем изменять функцию потерь от измененной $w$.

	Напишем идейный псевдокод.
	Скажем, что $x_{i} = \{x^{0}_{i}, ~ x^{1}_{i}, ~ \ldots, x^{n - 1}_{i}\}$~-- координата в $n$-мерном пространстве $\mathbb{R}^{n}$, $y_{i} = \{y^{0}_{i}, ~ y^{1}_{i}, ~ \ldots, y^{n - 1}_{i}\}$~-- образ функции $f(x_{i})$.
	\begin{mylisting}
function $\mathtt{S}(x, y, w)$:
	return $\sum\limits_{i = 1}^{N}{(y_{i} - x_{i} \cdot w_{i})^{2}}$

function $\mathtt{stochastic\_descent}(x, y)$:
	$w \gets$ [$w_{i} \in \mathbb{R}$] * n
	$\mathtt{prev} \gets INIT$, $\text{предыдущее значение функции потери}$
	$\mathtt{next} \gets S(x, y, w)$, $\text{текущее значение функции потери}$
	$\alpha \gets$ const
	while $|\mathtt{prev} - \mathtt{next}| > \varepsilon$:
		$\mathtt{prev} \gets \mathtt{next}$
		$i \gets x \in [0, ~ |Y|]$
		$T \gets$ [0] * n
		$\forall j \in |w|$ do
			$T_{j} \gets (y_{i} - x_{i} \times w) \cdot x_{i}^{j}$
		$w \gets w + \alpha \cdot T$
		$\mathtt{next} \gets S(x, y, w)$
	return $w$
	\end{mylisting}
	\subsubsection*{Minibatch градиентный спуск}
	Модификация \textit{Minibatch} обобщает вариант стохастического градиентного спуска, тем, что во время итерации мы будем брать не одну случайную точку из посчитанных на предыдущем шаге и изменять функцию потери как бы относительно её, а теперь возьмем выборку $M' \subset M$, причем, обязательно, чтобы $|M'| > 1$ и $|M'| < |M|$.

	Тогда псевдокод от предыдущего рассмотренного варианта почти ничем не отличается.
	Скажем также, что $x_{i} = \{x^{0}_{i}, ~ x^{1}_{i}, ~ \ldots, x^{n - 1}_{i}\}$~-- координата в $n$-мерном пространстве $\mathbb{R}^{n}$, $y_{i} = \{y^{0}_{i}, ~ y^{1}_{i}, ~ \ldots, y^{n - 1}_{i}\}$~-- образ функции $f(x_{i})$; значение $m$~-- выступает в роли мощности подмножества $M'$.
	\begin{mylisting}
function $\mathtt{S}(x, y, w)$:
	return $\sum\limits_{i = 1}^{N}{(y_{i} - x_{i} \cdot w_{i})^{2}}$
	
function $\mathtt{stochastic\_descent}(x, y, m)$:
	$w \gets$ [$w_{i} \in \mathbb{R}$] * n
	$\mathtt{prev} \gets INIT$, $\text{предыдущее значение функции потери}$
	$\mathtt{next} \gets S(x, y, w)$, $\text{текущее значение функции потери}$
	$\alpha \gets$ const
	while $|\mathtt{prev} - \mathtt{next}| > \varepsilon$:
		$\mathtt{prev} \gets \mathtt{next}$
		$\forall i \in [0, m]$ do
			$T \gets$ [0] * n
			$\forall j \in |w|$ do
				$T_{j} \gets (y_{i} - x_{i} \times w) \cdot x_{i}^{j}$
			$w \gets w + \alpha \cdot T$
		$\mathtt{next} \gets S(x, y, w)$
	return $w$
	\end{mylisting}
	\subsubsection*{Градиентный спуск}
	Наконец, самый общий случай и являющийся самым быстроходным среди двух рассмотренных ранее, благодаря тому, что мы учитываем все точки $M' \equiv M$.
	Данный метод работает крайне медленно, но является одним из самых быстрых в сходимости к приближенной точке минимума.

	Пусть $m$~-- есть мощность множества $M$, тогда идейным псевдокодом-решением задачи будет являться тот же код, что и для предыдущего варианта, то есть
	\begin{mylisting}
function $\mathtt{S}(x, y, w)$:
	return $\sum\limits_{i = 1}^{N}{(y_{i} - x_{i} \cdot w_{i})^{2}}$
	
function $\mathtt{stochastic\_descent}(x, y, m)$:
	$w \gets$ [$w_{i} \in \mathbb{R}$] * n
	$\mathtt{prev} \gets INIT$, $\text{предыдущее значение функции потери}$
	$\mathtt{next} \gets S(x, y, w)$, $\text{текущее значение функции потери}$
	$\alpha \gets$ const
	while $|\mathtt{prev} - \mathtt{next}| > \varepsilon$:
		$\mathtt{prev} \gets \mathtt{next}$
		$\forall i \in [0, m]$ do
			$T \gets$ [0] * n
			$\forall j \in |w|$ do
				$T_{j} \gets (y_{i} - x_{i} \times w) \cdot x_{i}^{j}$
		$w \gets w + \alpha \cdot T$
	$\mathtt{next} \gets S(x, y, w)$
	return $w$
	\end{mylisting}
	\subsection*{Learning rate scheduling}
	Задачей мы поставим подбор функции изменения шага, чтобы улучшить сходимость из предыдущего пункта.
	Тогда, мы использовали самый простой способ~-- \textit{константный}, однако у него есть недостаток: иногда шаг в $\mathtt{const\_learning\_rate}$ может <<перепрыгнуть>> через минимум и/или начать <<прыгать>> через него бесконечно много раз, в таком случае мы хотим, чтобы шаг был уменьшен.
	Другой случай: когда такой шаг может привести к долгому ожиданию, пока алгоритм не дойдет до минимума.
	Вот тут и возникает задача подбора такой функции изменения шага, чтобы алгоритм за какое-то конечное $k$ эпох добрался до минимума быстрее. 
	В качестве такой мы возьмем \textit{экспоненциальную функцию}.
	Экспоненциальная функция, в общем случае, для изменения сходимости выглядит так:
	\[
		\mathtt{learning\_rate} = \mathtt{start\_learning\_rate} \cdot e^{-k \cdot \mathtt{epoch}},
	\] где $\mathtt{learning\_rate}$~-- текущий размер шага, $\mathtt{start\_learning\_rate}$~-- стартовая длина, $k$~-- некий параметр, который может зависеть от размера выбранного батча, и, наконец, $\mathtt{epoch}$~-- текущий номер эпохи.
	\subsection*{Исследование различных модификаций}
	Исследуйте модификации градиентного спуска (Nesterov, Momentum, AdaGrad, RMSProp, Adam).
	\subsubsection*{Momentum}
	Пусть нам дана функция $f(x_{i})$, где $x_{i} = \{x_{i}^{1}, ~ x_{i}^{2}, ~ \ldots, ~ x_{i}^{n}\}$~-- координата точки $x_{i}$ в $n$-мерном пространстве~-- и пусть у данной $f$ есть множество локальных точек минимума, образующийся <<впадиной>> функции, и <<горы>>~-- максимумов.
	Рассмотрим некоторый \textit{объект} $\mathfrak{O}$, который обладает свойством <<скольжения>> по поверхности нашей функции $f$, определяемый следующим образом:
	\begin{enumerate}[a)]
		\item если он <<сходит>> с горы, то он не останавливается при <<схождении>> с локального максимума;
		\item если он <<перескакивает>> локальный минимум, то он либо остановится на идеально гладкой части поверхности функции $f$, либо он <<сойдет>> к локальному минимуму.
	\end{enumerate}
	Идея модификации градиентного спуска \textit{Momentum} в создании алгоритма, обладающий свойством $\mathfrak{O}$ в имении свойства импульса.

	Рассмотрим последовательность $\{u_{i}\}$, которую мы назовем <<скоростью>>, скажем, что $\{g_{i}\} \equiv \{-\nabla{f(x_{i})}\}$, тогда определим скорость следующим образом:
	\[
		v_{i + 1} = \lambda \cdot v_{i} + (1 - \lambda) \cdot g_{i}, ~ \lambda \in (0, 1)
	\] и зададим изменение на $i + 1$-ой итерации наших весов точек:
	\[
		w_{i + 1} = w_{i} - \alpha \cdot v_{i + 1}, ~ \alpha = \mathtt{const}
	\]
	\subsubsection*{Nesterov}
	Модификация \textit{Nesterov} по своей сути почти ничем не отличается ранее рассмотренного Momentum: единственное, что отделяет братьев по крови, так это то, что Nesterov считает градиент не в текущей точке, на которой мы стоим, а той, куда мы бы могли пойти, следуя импульсу.
	Тогда, наши основные формулы немного меняются, для скорости:
	\[
		v_{i + 1} = \lambda \cdot v_{i} + (1 - \lambda) \cdot g(w_{i} - \alpha \cdot \lambda \cdot v_{i}), ~ \alpha = \mathtt{const}
	\]
	И для весов:
	\[
		w_{i + 1} = w_{i} - \alpha \cdot v_{i + 1}
	\]
	\subsubsection*{AdaGrad}
	Рассмотрим некий $\mathtt{start\_learning\_rate}$ и стандартный метод стохастического спуска.
	Как мы уже видели, одним из недостатков такого метода является появления слишком больших или, наоборот, маленьких компонент относительно друг друга.
	Для исправления мы введём матрицу $\Omega$, которую определим следующим образом:
	\[
		\Omega = \sum\limits_{j = 1}^{i}{g_{j} \times g_{j}^{\mathrm{T}}},
	\] где $g_{j}$~-- $\nabla{f(x_{j})}$.
	Теперь рассмотрим основную формулу и поделим часть, где идет <<шаг>> алгоритма, на $\Omega_{i, i}$:
	\[
		w_{i + 1} = w_{i} - \dfrac{\alpha \cdot g_{j}}{\sqrt{\Omega_{i, i}}}
	\]
	Теперь же, там, где мы потенциально делаем большие шаги, мы делим на большое $\Omega_{i, i}$ и таким образом уменьшаем количество больших скачков.
	Тут мы получаем глобальную проблема в виде потенциально слишком высокой скорости уменьшения шага.
	\subsubsection*{RMSProp}
	Модификация \textit{RMSProp} решает возникшую проблему с AdaGrad, в качестве решения было предложено усреднять значения.
	Пусть $(g_{i})^{2}$~-- покомпонентное возведение в квадрат, $\eth$~-- некое число порядка $[10^{-10}, ~ 10^{-7}]$, дабы по формуле не было деления на ноль.
	Тогда, в качестве скорости возьмем:
	\[
		s_{i + 1} = \lambda \cdot s_{i} + (1 - \lambda) \cdot (g_{i})^{2}, ~ \lambda \in (0, 1)
	\]
	Наконец, изменение весов:
	\[
		w_{i + 1} = w_{i} - \alpha \cdot \dfrac{g_{i}}{\sqrt{s_{i + 1} + \eth}}, ~ \alpha = \mathtt{const}
	\]
	\subsubsection*{Adam}
	Начинаем объединение веселья предыдущих.
	В отличие от всех его предшественником модификация \textit{Adam} является наиболее распространенным и используемым.
	\begin{align*}
		v_{i + 1} &= \lambda_{1} \cdot v_{i} + (1 - \lambda_{1}) \cdot g_{i} \\
		s_{i + 1} &= \lambda_{2} \cdot s_{i} + (1 - \lambda_{2}) \cdot (g_{i})^{2} \\
		\hat{v}_{i + 1} &= \dfrac{v_{i + 1}}{1 - \lambda_{1}^{i + 1}} \\
		\hat{s}_{i + 1} &= \dfrac{s_{i + 1}}{1 - \lambda_{2}^{i + 1}}
	\end{align*}
	Наконец, изменение весов будем рассчитывать по следующей формуле:
	\[
		w_{i + 1} = w_{i} - \lambda \cdot \dfrac{\hat{v}_{i + 1}}{\sqrt{\hat{s}_{i + 1} + \eth}},
	\] где $\lambda_{1} = 0.9$, $\lambda_{2} = 0.999$ и $\eth = 10^{-8}$.
	\subsubsection*{Сравнение модификаций}
	\paragraph{Сходимость}
	\paragraph{Траектории}
	\newpage
	\section*{Полиномиальная регрессия}
	\subsection*{Введение}
	\subsection*{Исследование различных модификация}
	\subsubsection*{L1}
	\subsubsection*{L2}
	\subsubsection*{Elastic регуляризация}
\end{document}
