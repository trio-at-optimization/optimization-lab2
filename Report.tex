\documentclass[12pt, a4paper, oneside, final]{article}
\usepackage[margin = 1in, bottom = 1in]{geometry}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{xcolor, ulem, soulutf8, soul, fancyhdr, amsmath, amssymb, amsthm}
\usepackage[shortlabels]{enumitem}
\usepackage{titlesec, hyperref, multicol, listings}
\usepackage[most]{tcolorbox}
\usepackage[makeroom]{cancel}
\usepackage[most]{tcolorbox}
\usepackage{tocloft, longtable, skak, stmaryrd, color}
\usepackage[framemethod = tikz]{mdframed}
\usepackage{svg, wrapfig, csvsimple, float, caption, subcaption}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{blueish}{rgb}{0.96,0.96,1.0}
\definecolor{grayblueish}{rgb}{0.97,0.97,0.98}
\definecolor{transblue}{rgb}{0.9,0.9,0.97}
\definecolor{transred}{rgb}{0.97,0.9,0.9}
\definecolor{light-gray}{gray}{0.95}

\renewcommand{\figurename}{}
\addto\captionsrussian{\renewcommand{\figurename}{}}
\captionsetup[table]{labelformat = empty}

\hypersetup{
	colorlinks,
	citecolor = pink,
	filecolor = pink,
	linkcolor = pink,
	urlcolor = pink
}

\newtcblisting{mylisting}{
	listing only,
	breakable,
	colback=backcolour,
	enhanced jigsaw,
	sharp corners,
	boxrule=0pt,
	frame hidden,
	listing options={
		mathescape,
		commentstyle = \color{codegreen},
		keywordstyle = \color{magenta},
		numberstyle = \tiny\color{codegray},
		stringstyle = \color{codepurple},
		basicstyle = \ttfamily\footnotesize,
		breakatwhitespace = false,
		breaklines = true,
		captionpos = b,
		keepspaces = true,
		numbers = left,
		numbersep = 5pt,
		showspaces = false,
		showstringspaces = false,
		showtabs = false,
		tabsize = 4,
		inputencoding=utf8,
		language = python
	}
}

\binoppenalty = 10000
\relpenalty = 10000
\sloppy

\renewcommand*{\theenumi}{\thesection.\arabic{enumi}}
\renewcommand*{\theenumii}{\alph{enumii}}
\renewcommand*{\labelitemi}{\ensuremath{\triangleright}}

\everymath{\displaystyle}

\begin{document}
	\thispagestyle{empty}
	\vspace*{0.5em}
	\begin{center}
		{Национальный исследовательский университет ИТМО\\Факультет информационных технологий и программирования\\Прикладная математика и информатика}\\[5.0em]
		{\Huge \bfseries Методы оптимизации}\\[0.5em]
		{\large Отчет по лабораторной работе №2}\\[0.5em]
		\textcolor{gray}{\textlangle Собрано \today\textrangle}
	\end{center}
	\begingroup
	\def\hd{\begin{tabular}{ll}
			\textbf{Работу выполнили:} \\ Бактурин Савелий Филиппович M32331 \\ Вереня Андрей Тарасович M32331 \\ Сотников Максим Владимирович M32331 \vspace*{1em} \\
			\textbf{Преподаватель:} \\ TBA
		\end{tabular}
	}
	\vspace*{30em}
	\newlength{\hdwidth}
	\settowidth{\hdwidth}{\hd}
	\hfill\begin{minipage}{\hdwidth}\hd\end{minipage}
	\endgroup
	\newpage
	\section*{Стохастический градиентный спуск}
	\subsection*{Исследование с разными размерами батча}
	Реализуйте стохастический градиентный спуск для решения линейной регрессии. Исследуйте сходимость с разным размером батча ($1$~-- SGD, $2$, $\dots$, $n - 1$~-- Minibatch GD, $n$~-- GD из предыдущей работы).
	\subsubsection*{Стохастический градиентный спуск}
	\textit{Стохастический градиентный спуск}~--- модификация к основному методу итерационного поиска минимума через антиградиент дифференцируемой функции в рассматриваемой плоскости $\mathbb{R}^{n}$.
	Идея: пусть есть множество $M$~-- какой-то полный набор данных вычисленных оценок при спуске к минимуму, тогда в рассматриваемой версии мы будем брать случайное значение из выбранного $M' \subset M$.
	Как правило, такой подход преуменьшает вычислительные ресурсы, в особенности, следует помнить, что $\mathtt{float}$ считается крайне медленно, и ускоряет итерацию по количествам эпохам, но при этом мы теряем точность сходимости.

	Пусть $x_{i} = \{x_i^0, x_i^1, ~ \ldots, x_i^{n - 1}\}$~-- координата в $\mathbb{R}^{n}$ и задана функция $f(x_i) : \mathbb{R}^{n} \to \mathbb{R}$. Мы хотим нашу задачу свести к исследованию на некоторых специальных образцах заданной функции, для каждой точки из которых мы будем минимизировать ошибку для дальнейшего нахождения приближенного минимума рассматриваемой функции $f(x_i)$. Мы хотим найти линейную регрессию, представляющая из себя полином $1$-ой степени от $n$ переменных. Для начала мы найдем функцию ошибки $S$ по следующей формуле:
	\[
		S(f) = (X'^{\mathrm{T}} \times X')^{-1} \times X'^{\mathrm{T}} \times Y \times \vec{x},
	\] где $Y$~-- матрица значений при множестве $X$ (определение), $X'$~-- это матрица $X$, но в $1$-ой колонке забитый единицами. По другому мы можем записать данную формулу следующим образом:
	\[
		S(f) = \sum\limits_{i = 1}^{N}{\left(y_{i} - x_{i} \cdot w_{i}\right)^{2}},
	\] где $y_{i} \in D(f(x_{i}))$ (образ функции), $w_{i} \in W$~-- сгенерированные веса, коэффициенты при линейной функции.

	Рассмотрим идею алгоритма. При итерации, пока мы не превысили максимальное количество шагов или не сведем нашу функцию потери до некоторого $\varepsilon$, мы будем обобщать экспериментальные данные в виде случайных точек в некоторую многомерную линию. На каждом шаге мы будем изменять функцию потерь от измененной $w$.

	Напишем идейный псевдокод. Скажем, что $x_{i} = \{x^{0}_{i}, ~ x^{1}_{i}, ~ \ldots, x^{n - 1}_{i}\}$~-- координата в $n$-мерном пространстве $\mathbb{R}^{n}$, $y_{i} = \{y^{0}_{i}, ~ y^{1}_{i}, ~ \ldots, y^{n - 1}_{i}\}$~-- образ функции $f(x_{i})$.
	\begin{mylisting}
function $\mathtt{S}(x, y, w)$:
	return $\sum\limits_{i = 1}^{N}{(y_{i} - x_{i} \cdot w_{i})^{2}}$

function $\mathtt{stochastic\_descent}(x, y)$:
	$w \gets$ [$w_{i} \in \mathbb{R}$] * n
	$\mathtt{prev} \gets INIT$, $\text{предыдущее значение функции потери}$
	$\mathtt{next} \gets S(x, y, w)$, $\text{текущее значение функции потери}$
	$\alpha \gets$ const
	while $|\mathtt{prev} - \mathtt{next}| > \varepsilon$:
		$\mathtt{prev} \gets \mathtt{next}$
		$i \gets x \in [0, ~ |Y|]$
		$T \gets$ [0] * n
		$\forall j \in |w|$ do
			$T_{j} \gets (y_{i} - x_{i} \times w) \cdot x_{i}^{j}$
		$w \gets w + \alpha \cdot T$
		$\mathtt{next} \gets S(x, y, w)$
	return $w$
	\end{mylisting}
	\subsubsection*{Minibatch градиентный спуск}
	Модификация \textit{Minibatch} обобщает вариант стохастического градиентного спуска, тем, что во время итерации мы будем брать не одну случайную точку из посчитанных на предыдущем шаге и изменять функцию потери как бы относительно её, а теперь возьмем выборку $M' \subset M$, причем, обязательно, чтобы $|M'| > 1$ и $|M'| < |M|$.

	Тогда псевдокод от предыдущего рассмотренного варианта почти ничем не отличается. Скажем также, что $x_{i} = \{x^{0}_{i}, ~ x^{1}_{i}, ~ \ldots, x^{n - 1}_{i}\}$~-- координата в $n$-мерном пространстве $\mathbb{R}^{n}$, $y_{i} = \{y^{0}_{i}, ~ y^{1}_{i}, ~ \ldots, y^{n - 1}_{i}\}$~-- образ функции $f(x_{i})$; значение $m$~-- выступает в роли мощности подмножества $M'$.
	\begin{mylisting}
function $\mathtt{S}(x, y, w)$:
	return $\sum\limits_{i = 1}^{N}{(y_{i} - x_{i} \cdot w_{i})^{2}}$
	
function $\mathtt{stochastic\_descent}(x, y, m)$:
	$w \gets$ [$w_{i} \in \mathbb{R}$] * n
	$\mathtt{prev} \gets INIT$, $\text{предыдущее значение функции потери}$
	$\mathtt{next} \gets S(x, y, w)$, $\text{текущее значение функции потери}$
	$\alpha \gets$ const
	while $|\mathtt{prev} - \mathtt{next}| > \varepsilon$:
		$\mathtt{prev} \gets \mathtt{next}$
		$\forall i \in [0, m]$ do
			$T \gets$ [0] * n
			$\forall j \in |w|$ do
				$T_{j} \gets (y_{i} - x_{i} \times w) \cdot x_{i}^{j}$
			$w \gets w + \alpha \cdot T$
		$\mathtt{next} \gets S(x, y, w)$
	return $w$
	\end{mylisting}
	\subsubsection*{Градиентный спуск}
	Наконец, самый общий случай и являющийся самым быстроходным среди двух рассмотренных ранее, благодаря тому, что мы учитываем все точки $M' \equiv M$. Данный метод работает крайне медленно, но является одним из самых быстрых в сходимости к приближенной точке минимума.

	Пусть $m$~-- есть мощность множества $M$, тогда идейным псевдокодом-решением задачи будет являться тот же код, что и для предыдущего варианта, то есть
	\begin{mylisting}
function $\mathtt{S}(x, y, w)$:
	return $\sum\limits_{i = 1}^{N}{(y_{i} - x_{i} \cdot w_{i})^{2}}$
	
function $\mathtt{stochastic\_descent}(x, y, m)$:
	$w \gets$ [$w_{i} \in \mathbb{R}$] * n
	$\mathtt{prev} \gets INIT$, $\text{предыдущее значение функции потери}$
	$\mathtt{next} \gets S(x, y, w)$, $\text{текущее значение функции потери}$
	$\alpha \gets$ const
	while $|\mathtt{prev} - \mathtt{next}| > \varepsilon$:
		$\mathtt{prev} \gets \mathtt{next}$
		$\forall i \in [0, m]$ do
			$T \gets$ [0] * n
			$\forall j \in |w|$ do
				$T_{j} \gets (y_{i} - x_{i} \times w) \cdot x_{i}^{j}$
		$w \gets w + \alpha \cdot T$
	$\mathtt{next} \gets S(x, y, w)$
	return $w$
	\end{mylisting}
	\subsection*{Learning rate scheduling}
	\subsection*{Исследование различных модификаций}
	\subsubsection*{Nesterov}
	\subsubsection*{Momentum}
	\subsubsection*{AdaGrad}
	\subsubsection*{RMSProp}
	\subsubsection*{Adam}
	\subsubsection*{Сравнение модификаций}
	\paragraph{Сходимость}
	\paragraph{Траектории}
	\newpage
	\section*{Полиномиальная регрессия}
	\subsection*{Введение}
	\subsection*{Исследование различных модификация}
	\subsubsection*{L1}
	\subsubsection*{L2}
	\subsubsection*{Elastic регуляризация}
\end{document}
